from __future__ import annotations

import math
import os
import shutil
import time
from collections import Counter
from contextlib import contextmanager, nullcontext
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import geopandas as gpd
import pandas as pd

from build_network import build_processed_network
import capacity_calculator
from main import (
    generate_infra_development,
    create_dev_id_lookup_table,
    create_travel_time_graphs,
    compute_tts,
    rearange_costs,
)
import paths
import settings
from random_scenarios import get_random_scenarios
from scoring import construction_costs, create_cost_and_benefit_df, discounting
from plots import plot_costs_benefits_example
import cost_parameters as cp
import network_plot


@dataclass
class CapacitySnapshot:
    """Container for capacity station/segment tables."""

    stations: pd.DataFrame
    segments: pd.DataFrame


@dataclass
class ServiceProposal:
    """Representation of a new or modified service generated by the infrastructure step."""

    dev_id: Optional[int]
    service: str
    frequency: float
    from_node: int
    to_node: int
    via: Optional[str]
    travel_time_min: Optional[float]
    path_length_m: Optional[float]


@dataclass
class InfrastructurePlan:
    """Container for newly proposed services and their source artefacts."""

    proposals: List[ServiceProposal]
    source_path: Optional[Path]


@dataclass
class SegmentCapacityIssue:
    start_node: int
    end_node: int
    capacity_tphpd: Optional[float]
    baseline_tphpd: Optional[float]
    required_total_tphpd: Optional[float]
    available_margin: Optional[float]
    status: str  # 'missing_segment', 'unknown_capacity', 'capacity_exceeded'


@dataclass
class ProposalEvaluation:
    proposal: ServiceProposal
    issues: List[SegmentCapacityIssue]
    segment_context: List[Dict[str, Any]]
    path_nodes: List[int]


@dataclass
class ScoredDevelopment:
    development_id: str
    best_net_benefit: float
    best_cba_ratio: float


@dataclass
class PipelineConfig:
    """Shared configuration prepared during Phase 0."""

    main_dir: Path
    cache_flags: Dict[str, bool]
    output_dirs: Dict[str, Path]
    master_workbook_path: Path
    runtimes_path: Path
    processed_points: Optional[gpd.GeoDataFrame] = None
    prep_workbook_path: Optional[Path] = None
    sections_workbook_path: Optional[Path] = None
    capacity_snapshot: Optional[CapacitySnapshot] = None
    sections_dataframe: Optional[pd.DataFrame] = None
    baseline_plot_paths: Dict[str, Path] = field(default_factory=dict)
    infrastructure_plan: Optional[InfrastructurePlan] = None
    net_benefit_threshold: float = 0.0
    scoring_results: List[ScoredDevelopment] = field(default_factory=list)
    benefit_summary: Optional[pd.DataFrame] = None
    capacity_evaluations: List[ProposalEvaluation] = field(default_factory=list)
    selected_proposals: List[ServiceProposal] = field(default_factory=list)
    intervention_outputs: List[Dict[str, Any]] = field(default_factory=list)


class RuntimeLog:
    """Helper to capture and persist per-phase runtimes."""

    def __init__(self) -> None:
        self.timings: Dict[str, float] = {}

    @contextmanager
    def track(self, label: str):
        start = time.perf_counter()
        try:
            yield
        finally:
            self.timings[label] = time.perf_counter() - start

    def write(self, target: Path) -> None:
        target.parent.mkdir(parents=True, exist_ok=True)
        with target.open("w", encoding="utf-8") as stream:
            for part, duration in self.timings.items():
                stream.write(f"{part}: {duration:.2f}s\n")


PIPELINE_CONFIG: Optional[PipelineConfig] = None
PIPELINE_RUNTIMES: Optional[RuntimeLog] = None


def _resolve_main_directory() -> Path:
    """Return the directory that should act as paths.MAIN for this run."""
    configured = Path(paths.MAIN)
    if configured.exists():
        return configured
    fallback = Path(__file__).resolve().parent
    print(
        f"Phase 0: paths.MAIN not found at '{paths.MAIN}'. Using repository root '{fallback}' instead."
    )
    return fallback


def _synchronise_capacity_paths(main_dir: Path) -> None:
    """Update capacity_calculator path globals to match the resolved main directory."""
    capacity_calculator.DATA_ROOT = main_dir / "data" / "Network"
    capacity_calculator.PROCESSED_ROOT = capacity_calculator.DATA_ROOT / "processed"
    capacity_calculator.CAPACITY_ROOT = capacity_calculator.DATA_ROOT / "capacity"


def initialize_phase_zero() -> Tuple[PipelineConfig, RuntimeLog]:
    """Phase 0: Change into paths.MAIN, prime config, and prepare runtime tracking."""
    global PIPELINE_CONFIG, PIPELINE_RUNTIMES
    if PIPELINE_CONFIG is not None and PIPELINE_RUNTIMES is not None:
        return PIPELINE_CONFIG, PIPELINE_RUNTIMES

    runtimes = RuntimeLog()
    with runtimes.track("Phase 0 – Setup & runtime tracking"):
        main_dir = _resolve_main_directory()
        os.chdir(main_dir)
        paths.MAIN = str(main_dir)
        _synchronise_capacity_paths(main_dir)
        capacity_calculator.ensure_output_directory()

        output_dirs = {
            "plots": main_dir / paths.PLOT_DIRECTORY,
            "scenario_plots": main_dir / paths.PLOT_SCENARIOS,
            "capacity": capacity_calculator.CAPACITY_ROOT,
        }
        for directory in output_dirs.values():
            directory.mkdir(parents=True, exist_ok=True)

        cache_flags = {
            "network": settings.use_cache_network,
            "developments": settings.use_cache_developments,
            "pt_catchment": settings.use_cache_pt_catchment,
            "catchment_od": settings.use_cache_catchmentOD,
            "stations_od": settings.use_cache_stationsOD,
            "traveltime_graph": settings.use_cache_traveltime_graph,
            "scenarios": settings.use_cache_scenarios,
            "tts": settings.use_cache_tts_calc,
        }

        master_workbook_path = capacity_calculator.capacity_output_path()
        config = PipelineConfig(
            main_dir=main_dir,
            cache_flags=cache_flags,
            output_dirs=output_dirs,
            master_workbook_path=master_workbook_path,
            runtimes_path=main_dir / "runtimes.txt",
        )

    PIPELINE_CONFIG = config
    PIPELINE_RUNTIMES = runtimes
    return config, runtimes


def _prep_workbook_path(master_path: Path) -> Path:
    return master_path.with_name(f"{master_path.stem}_prep{master_path.suffix}")


def _sections_workbook_path(master_path: Path) -> Path:
    return master_path.with_name(f"{master_path.stem}_sections{master_path.suffix}")


def run_phase_one(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 1 – Base Network Generation."""
    label = "Preprocess the network"
    with runtimes.track(label):
        processed_points = generate_base_network(
            use_cache=config.cache_flags.get("network", settings.use_cache_network)
        )
    point_count = len(processed_points) if processed_points is not None else 0
    print(f"Phase 1 complete: {point_count:,} processed points available.")
    config.processed_points = processed_points


def run_phase_two(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 2 – Capacity Workbook Export, Manual Checkpoint, Baseline Plots."""
    with runtimes.track("Import raw data / Generate OD matrix"):
        print("Phase 2: Building capacity workbook ...")
        stations_df, segments_df = capacity_calculator.build_capacity_tables()
        master_path = capacity_calculator.capacity_output_path()
        with pd.ExcelWriter(master_path, engine=capacity_calculator.EXCEL_ENGINE) as writer:
            stations_df.to_excel(writer, sheet_name="Stations", index=False)
            segments_df.to_excel(writer, sheet_name="Segments", index=False)

        config.master_workbook_path = master_path
        print(f"  Capacity workbook written to {master_path}.")
        prep_path = _prep_workbook_path(master_path)
        print(
            "\nPlease enrich the workbook (tracks, platforms, speeds, etc.) "
            f"and save it as '{prep_path.name}'."
        )

        while True:
            response = input("Have you created and saved the enriched prep workbook (y/n/abort)? ").strip().lower()
            if response in {"abort", "q", "quit"}:
                raise RuntimeError("Phase 2 aborted by user before loading the prep workbook.")
            if response not in {"y", "yes"}:
                print("  Waiting for confirmation... (type 'abort' to exit)")
                continue
            if not prep_path.exists():
                print(f"  Expected file not found at {prep_path}. Please save the enriched workbook and retry.")
                continue
            break

        stations_df = pd.read_excel(prep_path, sheet_name="Stations")
        segments_df = pd.read_excel(prep_path, sheet_name="Segments")

        config.prep_workbook_path = prep_path
        config.capacity_snapshot = CapacitySnapshot(stations=stations_df, segments=segments_df)

    print(
        f"Phase 2 checkpoint: snapshot loaded ({len(stations_df)} stations, {len(segments_df)} segments). "
        f"Prep workbook: {prep_path.name}."
    )

    if not config.prep_workbook_path:
        raise RuntimeError("Phase 2 plotting requires the enriched prep workbook.")

    outputs: Dict[str, Path] = {}
    with runtimes.track("Visualize base network"):
        workbook = str(config.prep_workbook_path)
        print("Phase 2: Generating baseline infrastructure plot ...")
        infra_path = network_plot.network_current_map(workbook_path=workbook)
        outputs["infrastructure_plot"] = infra_path

        print("Phase 2: Generating speed profile plot ...")
        speed_path = network_plot.plot_speed_profile_network(workbook_path=workbook)
        outputs["speed_plot"] = speed_path

        print("Phase 2: Generating service frequency plot ...")
        service_path = network_plot.plot_service_network(workbook_path=workbook)
        outputs["service_plot"] = service_path

    config.baseline_plot_paths.update(outputs)
    print("Phase 2 plotting complete. Baseline plots saved:")
    for name, path in outputs.items():
        print(f"  {name}: {path}")


def _apply_section_capacities(segments_df: pd.DataFrame, sections_df: pd.DataFrame) -> pd.DataFrame:
    if sections_df is None or sections_df.empty or segments_df is None or segments_df.empty:
        return segments_df

    segments = segments_df.copy()
    capacity_columns = [
        column
        for column in (
            "Capacity",
            "capacity_tphpd",
            "capacity_base_tphpd",
            "capacity_good_tphpd",
            "capacity_single_track_tphpd",
        )
        if column in sections_df.columns
    ]
    if not capacity_columns:
        return segments

    def _capacity_value(row) -> Optional[float]:
        for column in capacity_columns:
            value = row.get(column) if isinstance(row, dict) else getattr(row, column, None)
            numeric = _as_optional_float(value)
            if numeric is not None:
                return numeric
        return None

    capacity_map: Dict[Tuple[int, int], float] = {}
    for row in sections_df.to_dict(orient="records"):
        capacity_value = _capacity_value(row)
        if capacity_value is None:
            continue
        sequence = row.get("segment_sequence") or ""
        tokens = [token.strip() for token in sequence.split("|") if token.strip() and "-" in token]
        for token in tokens:
            start_str, end_str = token.split("-", 1)
            try:
                start = int(start_str)
                end = int(end_str)
            except ValueError:
                continue
            key = (min(start, end), max(start, end))
            capacity_map[key] = capacity_value

    if not capacity_map:
        return segments

    def _segment_key(row: pd.Series) -> Optional[Tuple[int, int]]:
        try:
            start = int(row.get("from_node"))
            end = int(row.get("to_node"))
        except (TypeError, ValueError):
            return None
        return (min(start, end), max(start, end))

    segments["_segment_key"] = segments.apply(_segment_key, axis=1)
    segments["Capacity"] = segments.get("Capacity") if "Capacity" in segments.columns else float("nan")

    def _capacity_lookup(row: pd.Series) -> Optional[float]:
        key = row["_segment_key"]
        if key is None:
            return row.get("Capacity")
        capacity_value = capacity_map.get(key)
        if capacity_value is not None:
            return capacity_value
        return row.get("Capacity")

    segments["Capacity"] = segments.apply(_capacity_lookup, axis=1)
    segments = segments.drop(columns=["_segment_key"], errors="ignore")
    return segments


def _export_sections_workbook(
    stations_df: pd.DataFrame,
    segments_df: pd.DataFrame,
    sections_path: Path,
) -> pd.DataFrame:
    sections_df = capacity_calculator._build_sections_dataframe(stations_df, segments_df)
    sections_path.parent.mkdir(parents=True, exist_ok=True)
    sections_engine = capacity_calculator.APPEND_ENGINE or capacity_calculator.EXCEL_ENGINE
    with pd.ExcelWriter(sections_path, engine=sections_engine) as writer:
        stations_df.to_excel(writer, sheet_name="Stations", index=False)
        segments_df.to_excel(writer, sheet_name="Segments", index=False)
        if not sections_df.empty:
            sections_df.to_excel(writer, sheet_name="Sections", index=False)
    return sections_df


def run_phase_three(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 3 – Sections Visualization."""
    if not config.prep_workbook_path:
        raise RuntimeError("Phase 3 requires the enriched prep workbook from Phase 2.")

    label = "Sections export & plot"
    sections_path = _sections_workbook_path(config.master_workbook_path)
    with runtimes.track(label):
        stations_df = (
            config.capacity_snapshot.stations
            if config.capacity_snapshot is not None
            else pd.read_excel(config.prep_workbook_path, sheet_name="Stations")
        )
        segments_df = (
            config.capacity_snapshot.segments
            if config.capacity_snapshot is not None
            else pd.read_excel(config.prep_workbook_path, sheet_name="Segments")
        )

        sections_df = capacity_calculator._build_sections_dataframe(stations_df, segments_df)
        if sections_df.empty:
            raise ValueError(
                "Generated sections dataframe is empty. Ensure the prep workbook contains required columns."
            )

        sections_path.parent.mkdir(parents=True, exist_ok=True)
        sections_engine = capacity_calculator.APPEND_ENGINE or capacity_calculator.EXCEL_ENGINE
        with pd.ExcelWriter(sections_path, engine=sections_engine) as writer:
            stations_df.to_excel(writer, sheet_name="Stations", index=False)
            segments_df.to_excel(writer, sheet_name="Segments", index=False)
            sections_df.to_excel(writer, sheet_name="Sections", index=False)

        base_plot_path = config.baseline_plot_paths.get("infrastructure_plot")
        output_path = str(base_plot_path) if base_plot_path else None
        _, capacity_plot_path = network_plot.plot_capacity_network(
            workbook_path=str(config.prep_workbook_path),
            sections_workbook_path=str(sections_path),
            output_path=output_path,
            generate_network=base_plot_path is None,
        )

    config.sections_workbook_path = sections_path
    config.sections_dataframe = sections_df
    config.baseline_plot_paths["capacity_plot"] = capacity_plot_path

    if config.capacity_snapshot is not None:
        config.capacity_snapshot = CapacitySnapshot(
            stations=config.capacity_snapshot.stations,
            segments=_apply_section_capacities(config.capacity_snapshot.segments, sections_df),
        )

    print(
        f"Phase 3 complete: sections workbook saved to {sections_path}, capacity plot saved to {capacity_plot_path}."
    )


def run_phase_five(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 5 – Infrastructure Proposal Harvesting."""
    label = "Generate infrastructure developments"
    with runtimes.track(label):
        plan = generate_infrastructure_plan()

    config.infrastructure_plan = plan

    proposal_count = len(plan.proposals)
    unique_services = len({proposal.service for proposal in plan.proposals if proposal.service})
    unique_dev_ids = len({
        _normalise_dev_id(proposal.dev_id) for proposal in plan.proposals if _normalise_dev_id(proposal.dev_id)
    })

    print(
        f"Phase 5 complete: {proposal_count} proposals harvested "
        f"({unique_services} distinct services, {unique_dev_ids} development IDs)."
    )
    if plan.source_path:
        print(f"  Source file: {plan.source_path}")
    if proposal_count:
        preview = plan.proposals[:5]
        print("  Sample proposals:")
        for entry in preview:
            print(
                f"    dev={entry.dev_id} service={entry.service or 'Unnamed'} "
                f"{entry.from_node}->{entry.to_node} freq={entry.frequency:.2f} tph"
            )
        if proposal_count > len(preview):
            print(f"    ... and {proposal_count - len(preview)} more proposals.")


def run_phase_six(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 6 – Scenario & Economic Evaluation (Net-Benefit Gate)."""
    if not config.infrastructure_plan or not config.infrastructure_plan.proposals:
        print("Phase 6: No proposals available to score. Ensure Phase 5 completed successfully.")
        return

    default_threshold = config.net_benefit_threshold or 0.0
    prompt = input(
        f"Phase 6: Enter net-benefit threshold in CHF (default {default_threshold:.0f}, negatives allowed): "
    ).strip()
    if prompt:
        try:
            threshold = float(prompt)
        except ValueError:
            print("  Invalid number; falling back to default threshold 0.")
            threshold = 0.0
    else:
        threshold = default_threshold

    plan, scored, summary_df = score_developments(
        config.infrastructure_plan,
        net_benefit_threshold=threshold,
        runtimes=runtimes,
    )

    config.infrastructure_plan = plan
    config.scoring_results = scored
    config.net_benefit_threshold = threshold
    config.benefit_summary = summary_df

    retained = summary_df[summary_df["retained"]] if not summary_df.empty else pd.DataFrame()
    mean_net = float(retained["best_net_benefit"].mean()) if not retained.empty else float("nan")
    mean_cba = float(retained["best_cba_ratio"].mean()) if not retained.empty else float("nan")

    summary_dir = Path(paths.MAIN) / "data" / "costs"
    summary_dir.mkdir(parents=True, exist_ok=True)
    summary_path = summary_dir / "net_benefit_summary.csv"
    if not summary_df.empty:
        summary_df.to_csv(summary_path, index=False)

    print("Phase 6 complete: net-benefit screening summary")
    print(f"  Threshold: {threshold:,.0f} CHF")
    print(f"  Proposals retained: {len(plan.proposals)}")
    print(f"  Mean net benefit (retained): {mean_net:,.0f} CHF")
    print(f"  Mean CBA ratio (retained): {mean_cba:.2f}")
    if summary_df.empty:
        print("  No scoring results to summarize.")
    else:
        print(f"  Summary CSV: {summary_path}")


def run_phase_seven(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 7 – Capacity Screening & Manual Checkpoint."""
    if not config.infrastructure_plan or not config.infrastructure_plan.proposals:
        print("Phase 7: No proposals available for capacity evaluation. Skipping.")
        return
    if not config.capacity_snapshot:
        raise RuntimeError("Phase 7 requires the Phase 2 capacity snapshot.")

    label = "Capacity evaluation"
    with runtimes.track(label):
        evaluations = evaluate_capacity(config.infrastructure_plan, config.capacity_snapshot)

    config.capacity_evaluations = evaluations

    exceeded = [evaluation for evaluation in evaluations if evaluation.issues]
    ok_count = len(evaluations) - len(exceeded)
    print(
        f"Phase 7 complete: {len(evaluations)} proposals evaluated "
        f"({ok_count} OK, {len(exceeded)} requiring intervention)."
    )

    if exceeded:
        prompt = input(
            "Manual checkpoint: Have you updated the workbook/data to address capacity issues (y/n)? "
        ).strip().lower()
        if prompt in {"y", "yes"}:
            adjust = input("Would you like to iteratively resolve conflicts now? (y/n): ").strip().lower()
            if adjust in {"y", "yes"}:
                plan_after_resolution = resolve_capacity_conflicts(config.infrastructure_plan, config.capacity_snapshot)
                config.infrastructure_plan = plan_after_resolution
                evaluations = evaluate_capacity(plan_after_resolution, config.capacity_snapshot)
                config.capacity_evaluations = evaluations
                exceeded = [evaluation for evaluation in evaluations if evaluation.issues]
                ok_count = len(evaluations) - len(exceeded)
                print(
                    f"  Post-resolution status: {len(evaluations)} proposals (OK {ok_count}, issues {len(exceeded)})."
                )

    export_dir = Path(paths.MAIN) / "data" / "capacity"
    export_dir.mkdir(parents=True, exist_ok=True)
    evaluations_csv = export_dir / "capacity_evaluations.csv"
    rows: List[Dict[str, Any]] = []
    for evaluation in config.capacity_evaluations:
        status = "ok"
        if evaluation.issues:
            status = ",".join(sorted({issue.status for issue in evaluation.issues}))
        rows.append(
            {
                "dev_id": evaluation.proposal.dev_id,
                "service": evaluation.proposal.service,
                "frequency_tph": evaluation.proposal.frequency,
                "from_node": evaluation.proposal.from_node,
                "to_node": evaluation.proposal.to_node,
                "status": status,
                "issues_count": len(evaluation.issues),
                "path_nodes": "->".join(map(str, evaluation.path_nodes)),
            }
        )
    pd.DataFrame(rows).to_csv(evaluations_csv, index=False)
    print(f"  Capacity evaluation summary written to {evaluations_csv}")


def run_phase_eight(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 8 – User Selection of Interventions."""
    plan = config.infrastructure_plan
    if not plan or not plan.proposals:
        print("Phase 8: No proposals available for user selection.")
        return

    evaluations_lookup: Dict[Tuple[Optional[str], int, int, str], ProposalEvaluation] = {}
    for evaluation in config.capacity_evaluations:
        evaluations_lookup[_proposal_key(evaluation.proposal)] = evaluation

    print("Phase 8: Available proposals (after capacity/net-benefit screening):")
    for idx, proposal in enumerate(plan.proposals, start=1):
        key = _proposal_key(proposal)
        evaluation = evaluations_lookup.get(key)
        status = "ok"
        if evaluation and evaluation.issues:
            status = ",".join(sorted({issue.status for issue in evaluation.issues}))
        print(
            f"  [{idx}] dev={proposal.dev_id} service={proposal.service or 'Unnamed'} "
            f"{proposal.from_node}->{proposal.to_node} freq={proposal.frequency:.2f} tph (capacity={status})"
        )

    selection = input(
        "Enter comma-separated indices to simulate (blank to skip): "
    ).strip()
    if not selection:
        print("  No interventions selected.")
        config.selected_proposals = []
        return

    chosen: List[ServiceProposal] = []
    for token in selection.split(","):
        token = token.strip()
        if not token:
            continue
        try:
            idx = int(token)
        except ValueError:
            print(f"  Ignoring invalid selection '{token}'.")
            continue
        if 1 <= idx <= len(plan.proposals):
            chosen.append(plan.proposals[idx - 1])
        else:
            print(f"  Selection {idx} out of range; skipped.")

    config.selected_proposals = chosen
    with runtimes.track("User selection"):
        print(f"  {len(chosen)} interventions queued for Phase 9 simulation.")


def run_phase_nine(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 9 – Intervention Simulation Loop."""
    if not config.selected_proposals:
        print("Phase 9: No interventions selected; skipping simulation loop.")
        return
    if not config.prep_workbook_path:
        raise RuntimeError("Phase 9 requires the Phase 2 prep workbook as a template.")

    base_prep = Path(config.prep_workbook_path)
    interventions_root = Path(paths.MAIN) / "data" / "capacity" / "interventions"
    plot_root = Path(paths.MAIN) / "plots" / "network" / "interventions"

    for proposal in config.selected_proposals:
        dev_label = _normalise_dev_id(proposal.dev_id) or proposal.service or "custom"
        safe_label = str(dev_label).replace("/", "_")
        label = f"Intervention {safe_label} simulation"
        with runtimes.track(label):
            intervention_dir = interventions_root / safe_label
            intervention_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_prep_path = intervention_dir / f"{base_prep.stem}_{safe_label}_{timestamp}.xlsx"
            shutil.copy2(base_prep, temp_prep_path)

            stations_df = pd.read_excel(temp_prep_path, sheet_name="Stations")
            segments_df = pd.read_excel(temp_prep_path, sheet_name="Segments")

            sections_path = intervention_dir / f"{temp_prep_path.stem}_sections.xlsx"
            sections_df = _export_sections_workbook(stations_df, segments_df, sections_path)

            plot_dir = plot_root / safe_label
            plot_dir.mkdir(parents=True, exist_ok=True)
            infra_plot = network_plot.network_current_map(
                workbook_path=str(temp_prep_path),
                output_path=str(plot_dir / f"{safe_label}_network.png"),
            )
            speed_plot = network_plot.plot_speed_profile_network(
                workbook_path=str(temp_prep_path),
                output_path=str(plot_dir / f"{safe_label}_speed.png"),
            )
            service_plot = network_plot.plot_service_network(
                workbook_path=str(temp_prep_path),
                output_path=str(plot_dir / f"{safe_label}_service.png"),
            )
            _, capacity_plot = network_plot.plot_capacity_network(
                workbook_path=str(temp_prep_path),
                sections_workbook_path=str(sections_path),
                output_path=str(plot_dir / f"{safe_label}_capacity.png"),
                generate_network=False,
            )

            stations_csv = intervention_dir / "stations.csv"
            segments_csv = intervention_dir / "segments.csv"
            stations_df.to_csv(stations_csv, index=False)
            segments_df.to_csv(segments_csv, index=False)
            sections_csv_path = None
            if not sections_df.empty:
                sections_csv_path = intervention_dir / "sections.csv"
                sections_df.to_csv(sections_csv_path, index=False)

            config.intervention_outputs.append(
                {
                    "dev_id": dev_label,
                    "service": proposal.service,
                    "prep_workbook": temp_prep_path,
                    "sections_workbook": sections_path,
                    "plots": {
                        "infrastructure": infra_plot,
                        "speed": speed_plot,
                        "service": service_plot,
                        "capacity": capacity_plot,
                    },
                    "tables": {
                        "stations": stations_csv,
                        "segments": segments_csv,
                        "sections": sections_csv_path,
                    },
                }
            )

    print(
        f"Phase 9 complete: generated artefacts for {len(config.intervention_outputs)} selected interventions."
    )


def run_phase_ten(config: PipelineConfig, runtimes: RuntimeLog) -> None:
    """Phase 10 – Reporting & Wrap-Up."""
    with runtimes.track("Reporting & wrap-up"):
        report_dir = Path(paths.MAIN) / "data" / "reports"
        report_dir.mkdir(parents=True, exist_ok=True)
        report_path = report_dir / "capacity_pipeline_report.txt"

        lines: List[str] = []
        lines.append("InfraScan Rail Capacity Pipeline Report")
        lines.append(f"Generated: {datetime.now():%Y-%m-%d %H:%M:%S}")
        lines.append("")

        plan = config.infrastructure_plan
        retained_count = len(plan.proposals) if plan else 0
        lines.append(f"Net-benefit threshold: {config.net_benefit_threshold:,.0f} CHF")
        lines.append(f"Proposals retained after screening: {retained_count}")
        lines.append(f"Interventions selected for simulation: {len(config.selected_proposals)}")
        lines.append("")

        if config.benefit_summary is not None and not config.benefit_summary.empty:
            retained = config.benefit_summary[config.benefit_summary["retained"]]
            if not retained.empty:
                lines.append(
                    f"Mean net benefit (retained): {float(retained['best_net_benefit'].mean()):,.0f} CHF"
                )
                lines.append(
                    f"Mean CBA ratio (retained): {float(retained['best_cba_ratio'].mean()):.2f}"
                )
                lines.append("")

        status_counter = Counter()
        for evaluation in config.capacity_evaluations:
            status = "ok"
            if evaluation.issues:
                status = ",".join(sorted({issue.status for issue in evaluation.issues}))
            status_counter[status] += 1
        if status_counter:
            lines.append("Capacity evaluation summary:")
            for status, count in status_counter.items():
                lines.append(f"  {status}: {count}")
            lines.append("")

        lines.append("Key artefacts:")
        net_benefit_path = Path(paths.MAIN) / "data" / "costs" / "net_benefit_summary.csv"
        if net_benefit_path.exists():
            lines.append(f"  Net-benefit summary: {net_benefit_path}")
        capacity_csv = Path(paths.MAIN) / "data" / "capacity" / "capacity_evaluations.csv"
        if capacity_csv.exists():
            lines.append(f"  Capacity evaluation CSV: {capacity_csv}")
        for name, path in config.baseline_plot_paths.items():
            lines.append(f"  Plot ({name}): {path}")
        for entry in config.intervention_outputs:
            lines.append(
                f"  Intervention {entry['dev_id']} prep workbook: {entry['prep_workbook']}"
            )
            lines.append(
                f"    Sections: {entry['sections_workbook']}"
            )
            for plot_label, plot_path in entry["plots"].items():
                lines.append(f"    Plot ({plot_label}): {plot_path}")

        with report_path.open("w", encoding="utf-8") as stream:
            stream.write("\n".join(lines))

    print(f"Phase 10 complete: report written to {report_path}.")

def generate_base_network(use_cache: Optional[bool] = None) -> gpd.GeoDataFrame:
    """Phase 1: regenerate or load the processed baseline network."""
    if use_cache is None:
        use_cache = settings.use_cache_network
    network_name = getattr(settings, "rail_network", "unknown")
    print(f"    Active rail network: {network_name}")
    processed_points = build_processed_network(use_cache=use_cache)
    print("    Processed network artefacts written under data/Network/processed/")
    return processed_points


def calculate_capacity_snapshot() -> CapacitySnapshot:
    """Phase 2: compute station/segment metrics for the current network."""
    stations_df, segments_df = capacity_calculator.build_capacity_tables()
    print("    Capacity tables generated in-memory (no workbook export).")
    return CapacitySnapshot(stations=stations_df, segments=segments_df)


def _as_optional_float(value) -> Optional[float]:
    if value is None:
        return None
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return None
    if math.isnan(numeric):
        return None
    return numeric


def _normalise_dev_id(value: Optional[object]) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        candidate = value.strip()
        return candidate or None
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return str(value)
    if math.isnan(numeric):
        return None
    if numeric.is_integer():
        return str(int(numeric))
    return f"{numeric}"


def _proposal_key(proposal: ServiceProposal) -> Tuple[Optional[str], int, int, str]:
    return (
        _normalise_dev_id(proposal.dev_id),
        proposal.from_node,
        proposal.to_node,
        proposal.service or "",
    )


def _parse_path_nodes(proposal: ServiceProposal) -> List[int]:
    nodes = [proposal.from_node]
    via = (proposal.via or "").strip()
    if via and via not in {"-99", "-1", "[]"}:
        for token in via.replace("[", "").replace("]", "").split(","):
            token = token.strip()
            if not token:
                continue
            try:
                value = int(token)
            except ValueError:
                continue
            nodes.append(value)
    nodes.append(proposal.to_node)
    return nodes


def _build_segment_index(segments_df: pd.DataFrame) -> Dict[Tuple[int, int], pd.Series]:
    index: Dict[Tuple[int, int], pd.Series] = {}
    for row in segments_df.itertuples(index=False):
        start = int(getattr(row, "from_node"))
        end = int(getattr(row, "to_node"))
        key = (min(start, end), max(start, end))
        index[key] = pd.Series(row._asdict())
    return index


def _segment_context(
    proposal: ServiceProposal,
    segment_index: Dict[Tuple[int, int], pd.Series],
    capacity_column: Optional[str],
) -> Tuple[List[int], List[Dict[str, Any]]]:
    nodes = _parse_path_nodes(proposal)
    context: List[Dict[str, Any]] = []
    for start, end in zip(nodes, nodes[1:]):
        key = (min(start, end), max(start, end))
        row = segment_index.get(key)
        entry: Dict[str, Any] = {
            "start": start,
            "end": end,
            "baseline": None,
            "capacity": None,
            "required": None,
            "margin": None,
            "status": "missing_segment",
        }
        if row is None:
            context.append(entry)
            continue

        baseline = _as_optional_float(row.get("total_tphpd"))
        if baseline is None:
            baseline_value = 0.0
        else:
            baseline_value = baseline
        capacity_value = _as_optional_float(row.get(capacity_column)) if capacity_column else None
        required = baseline_value + proposal.frequency
        margin = capacity_value - baseline_value if capacity_value is not None else None

        if capacity_value is None:
            status = "unknown_capacity"
        elif required > capacity_value + 1e-6:
            status = "capacity_exceeded"
        else:
            status = "ok"

        entry.update(
            {
                "baseline": baseline_value,
                "capacity": capacity_value,
                "required": required,
                "margin": margin,
                "status": status,
            }
        )
        context.append(entry)
    return nodes, context


def generate_infrastructure_plan() -> InfrastructurePlan:
    """Run infrastructure generation and capture proposed services."""
    print("Generating infrastructure proposals ...")
    generate_infra_development(
        use_cache=settings.use_cache_developments,
        mod_type=settings.infra_generation_modification_type,
    )

    source_path = Path(paths.NEW_LINKS_UPDATED_PATH)
    proposals: List[ServiceProposal] = []
    if source_path.exists():
        new_links = gpd.read_file(source_path)
        for row in new_links.itertuples(index=False):
            service_name = getattr(row, "Sline", None) or getattr(row, "Service", None) or ""
            if isinstance(service_name, float) and math.isnan(service_name):
                service_name = ""
            frequency = getattr(row, "Frequency", None)
            if frequency is None or (isinstance(frequency, float) and math.isnan(frequency)):
                frequency = 2.0
            proposals.append(
                ServiceProposal(
                    dev_id=getattr(row, "dev_id", None),
                    service=str(service_name),
                    frequency=float(frequency),
                    from_node=int(getattr(row, "from_ID_new", getattr(row, "FromNode", -1))),
                    to_node=int(getattr(row, "to_ID", getattr(row, "ToNode", -1))),
                    via=str(getattr(row, "Via", getattr(row, "via_nodes", ""))),
                    travel_time_min=_as_optional_float(getattr(row, "time", None)),
                    path_length_m=_as_optional_float(getattr(row, "shortest_path_length", None)),
                )
            )
        print(f"    Retrieved {len(proposals)} proposed services from {source_path}.")
    else:
        print("    No new links file was produced; proposals list is empty.")

    return InfrastructurePlan(proposals=proposals, source_path=source_path if source_path.exists() else None)


def score_developments(
    plan: InfrastructurePlan,
    *,
    net_benefit_threshold: float = 0.0,
    runtimes: Optional[RuntimeLog] = None,
    plot_limit: int = 5,
) -> Tuple[InfrastructurePlan, List[ScoredDevelopment], pd.DataFrame]:
    """Evaluate developments using the existing scoring pipeline with threshold filtering."""
    if not plan.proposals:
        print("Phase 6: No infrastructure proposals to score.")
        return plan, [], pd.DataFrame()

    print("Phase 6: Scoring infrastructure proposals ...")

    def _track(label: str):
        return runtimes.track(label) if runtimes else nullcontext()

    dev_lookup = create_dev_id_lookup_table()
    if dev_lookup.empty:
        print("    Development lookup table is empty; skipping benefit screening.")
        return plan, [], pd.DataFrame()

    dev_lookup.index = dev_lookup.index.astype(int)

    with _track("Calculate Traveltimes for all developments"):
        od_times_dev, od_times_status_quo, _, _ = create_travel_time_graphs(
            settings.rail_network,
            settings.use_cache_traveltime_graph,
            dev_lookup,
        )

    if settings.OD_type == "canton_ZH":
        with _track("Generate the scenarios"):
            get_random_scenarios(
                start_year=2018,
                end_year=2100,
                num_of_scenarios=settings.amount_of_scenarios,
                use_cache=settings.use_cache_scenarios,
                do_plot=False,
            )

    with _track("Calculate the TTT savings"):
        try:
            compute_tts(
                dev_lookup,
                od_times_dev,
                od_times_status_quo,
                use_cache=settings.use_cache_tts_calc,
            )
        except FileNotFoundError:
            compute_tts(
                dev_lookup,
                od_times_dev,
                od_times_status_quo,
                use_cache=False,
            )

    with _track("Compute costs"):
        construction_costs(
            file_path="data/Network/Rail-Service_Link_construction_cost.csv",
            cost_per_meter=cp.track_cost_per_meter,
            tunnel_cost_per_meter=cp.tunnel_cost_per_meter,
            bridge_cost_per_meter=cp.bridge_cost_per_meter,
            track_maintenance_cost=cp.track_maintenance_cost,
            tunnel_maintenance_cost=cp.tunnel_maintenance_cost,
            bridge_maintenance_cost=cp.bridge_maintenance_cost,
            duration=cp.duration,
        )

        costs_and_benefits = create_cost_and_benefit_df(
            settings.start_year_scenario,
            settings.end_year_scenario,
            settings.start_valuation_year,
        )
        discounted_costs = discounting(
            costs_and_benefits,
            discount_rate=cp.discount_rate,
            base_year=settings.start_valuation_year,
        )
        discounted_costs.to_csv(paths.COST_AND_BENEFITS_DISCOUNTED, index=False)

    with _track("Aggregate costs"):
        rearange_costs(discounted_costs)

    total_costs_path = Path(paths.TOTAL_COST_RAW)
    if not total_costs_path.exists():
        print(f"    Warning: total cost file not found at {total_costs_path}; skipping filtering.")
        return plan, [], pd.DataFrame()

    total_costs = pd.read_csv(total_costs_path)
    total_costs["total_costs"] = (
        total_costs.get("construction_cost", 0.0)
        + total_costs.get("maintenance_cost", 0.0)
        + total_costs.get("uncovered_op_cost", 0.0)
    )
    total_costs["net_benefit"] = total_costs.get("monetized_savings_total", 0.0) - total_costs["total_costs"]
    total_costs["cba_ratio"] = total_costs.get("monetized_savings_total", 0.0) / total_costs["total_costs"].replace(0, pd.NA)

    scored: List[ScoredDevelopment] = []
    summary_rows: List[Dict[str, Any]] = []
    retained_ids: set[str] = set()

    for development, group in total_costs.groupby("development"):
        dev_key = _normalise_dev_id(development)
        if dev_key is None:
            continue
        best_net = float(group["net_benefit"].max())
        best_ratio = float(group["cba_ratio"].max(skipna=True))
        scored.append(
            ScoredDevelopment(
                development_id=dev_key,
                best_net_benefit=best_net,
                best_cba_ratio=best_ratio if not math.isnan(best_ratio) else float("nan"),
            )
        )
        retained = best_net >= net_benefit_threshold
        if retained:
            retained_ids.add(dev_key)
            if dev_key.isdigit():
                retained_ids.add(f"{dev_key}.0")
        summary_rows.append(
            {
                "development_id": dev_key,
                "best_net_benefit": best_net,
                "best_cba_ratio": best_ratio if not math.isnan(best_ratio) else math.nan,
                "retained": retained,
            }
        )

    summary_df = pd.DataFrame(summary_rows)

    def _proposal_viable(proposal: ServiceProposal) -> bool:
        dev_id = _normalise_dev_id(proposal.dev_id)
        if dev_id is None:
            return False
        return dev_id in retained_ids

    filtered_proposals = [proposal for proposal in plan.proposals if _proposal_viable(proposal)]
    dropped = len(plan.proposals) - len(filtered_proposals)
    print(
        f"    {len(filtered_proposals)} proposals retained after applying threshold {net_benefit_threshold:.2f} "
        f"(dropped {dropped})."
    )

    retained_summary = summary_df[summary_df["retained"]]
    if not retained_summary.empty:
        top_entries = retained_summary.sort_values("best_net_benefit", ascending=False).head(5)
        print("  Top retained developments:")
        for _, row in top_entries.iterrows():
            print(
                f"    Development {row['development_id']}: net benefit {row['best_net_benefit']:,.0f} CHF, "
                f"CBA ratio {row['best_cba_ratio']:.2f}"
            )
        retained_ids_sorted = (
            retained_summary.sort_values("best_net_benefit", ascending=False)["development_id"].tolist()
        )
        for dev_id in retained_ids_sorted[:max(1, min(plot_limit, len(retained_ids_sorted)))]:
            plot_costs_benefits_example(discounted_costs, line=str(dev_id))

    return (
        InfrastructurePlan(proposals=filtered_proposals, source_path=plan.source_path),
        scored,
        summary_df,
    )


def evaluate_capacity(plan: InfrastructurePlan, snapshot: CapacitySnapshot) -> List[ProposalEvaluation]:
    """Phase 5: validate infrastructure proposals against baseline capacity."""
    if not plan.proposals:
        print("Phase 5: No infrastructure proposals to validate.")
        return []

    segments_df = snapshot.segments.copy()
    capacity_column = None
    for candidate in ("capacity_tphpd", "Capacity"):
        if candidate in segments_df.columns:
            capacity_column = candidate
            break

    if capacity_column is None:
        print(
            "Phase 5: Skipping capacity validation - no capacity column found "
            "(expected 'capacity_tphpd' or 'Capacity')."
        )
        return []

    segment_index = _build_segment_index(segments_df)
    evaluations: List[ProposalEvaluation] = []
    exceeded_count = missing_count = unknown_count = 0

    for proposal in plan.proposals:
        path_nodes, context = _segment_context(proposal, segment_index, capacity_column)
        issues: List[SegmentCapacityIssue] = []

        for entry in context:
            status = entry["status"]
            if status == "missing_segment":
                issues.append(
                    SegmentCapacityIssue(
                        start_node=entry["start"],
                        end_node=entry["end"],
                        capacity_tphpd=None,
                        baseline_tphpd=None,
                        required_total_tphpd=None,
                        available_margin=None,
                        status=status,
                    )
                )
                missing_count += 1
            elif status == "unknown_capacity":
                issues.append(
                    SegmentCapacityIssue(
                        start_node=entry["start"],
                        end_node=entry["end"],
                        capacity_tphpd=None,
                        baseline_tphpd=entry["baseline"],
                        required_total_tphpd=None,
                        available_margin=None,
                        status=status,
                    )
                )
                unknown_count += 1
            elif status == "capacity_exceeded":
                issues.append(
                    SegmentCapacityIssue(
                        start_node=entry["start"],
                        end_node=entry["end"],
                        capacity_tphpd=entry["capacity"],
                        baseline_tphpd=entry["baseline"],
                        required_total_tphpd=entry["required"],
                        available_margin=entry["margin"],
                        status=status,
                    )
                )
                exceeded_count += 1

        evaluations.append(
            ProposalEvaluation(
                proposal=proposal,
                issues=issues,
                segment_context=context,
                path_nodes=path_nodes,
            )
        )

    print(
        f"Phase 5: Evaluated {len(plan.proposals)} proposals "
        f"(segments over capacity: {exceeded_count}, missing segments: {missing_count}, unknown capacities: {unknown_count})."
    )

    return evaluations


def _safe_station_label(value: Any) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        text = value.strip()
        return text or None
    if pd.isna(value):
        return None
    text = str(value).strip()
    return text or None


def _build_station_lookup(stations_df: pd.DataFrame) -> Dict[int, str]:
    lookup: Dict[int, str] = {}
    if stations_df is None or stations_df.empty:
        return lookup
    for row in stations_df.itertuples(index=False):
        nr = getattr(row, "NR", None)
        if nr is None or pd.isna(nr):
            continue
        try:
            node_id = int(nr)
        except (TypeError, ValueError):
            continue
        code = _safe_station_label(getattr(row, "CODE", None))
        name = _safe_station_label(getattr(row, "NAME", None))
        label = code or name
        if label:
            lookup[node_id] = label
    return lookup


def _format_node(node_id: int, station_lookup: Dict[int, str]) -> str:
    label = station_lookup.get(node_id)
    return f"{label} [{node_id}]" if label else str(node_id)


def _proposal_with_nodes(proposal: ServiceProposal, nodes: List[int]) -> ServiceProposal:
    if len(nodes) < 2:
        raise ValueError("A shortened service must contain at least two stations.")
    via_nodes = nodes[1:-1]
    if via_nodes:
        via_str = "[" + ",".join(str(n) for n in via_nodes) + "]"
    else:
        via_str = "-99"
    return ServiceProposal(
        dev_id=proposal.dev_id,
        service=proposal.service,
        frequency=proposal.frequency,
        from_node=nodes[0],
        to_node=nodes[-1],
        via=via_str,
        travel_time_min=None,
        path_length_m=None,
    )


def _resolve_single_capacity_issue(
    evaluation: ProposalEvaluation,
    proposals: List[ServiceProposal],
    station_lookup: Dict[int, str],
) -> List[ServiceProposal]:
    try:
        index = proposals.index(evaluation.proposal)
    except ValueError:
        index = next((i for i, proposal in enumerate(proposals) if proposal == evaluation.proposal), None)
    if index is None:
        return proposals

    proposal = proposals[index]
    path_text = " -> ".join(_format_node(node_id, station_lookup) for node_id in evaluation.path_nodes)
    print("\nCapacity conflict detected:")
    print(f"  Service: {proposal.service or 'Unnamed'} (dev {proposal.dev_id}, freq {proposal.frequency:.2f} tph)")
    print(f"  Route:   {path_text}")

    for issue in evaluation.issues:
        start_label = _format_node(issue.start_node, station_lookup)
        end_label = _format_node(issue.end_node, station_lookup)
        if issue.status == "capacity_exceeded":
            print(
                f"    {start_label} -> {end_label}: requires {issue.required_total_tphpd:.2f} tph "
                f"vs capacity {issue.capacity_tphpd:.2f} (baseline {issue.baseline_tphpd:.2f})"
            )
        elif issue.status == "missing_segment":
            print(f"    {start_label} -> {end_label}: missing from baseline capacity table.")
        elif issue.status == "unknown_capacity":
            print(f"    {start_label} -> {end_label}: capacity value unavailable.")

    capacity_margins = [
        entry["margin"]
        for entry in evaluation.segment_context
        if entry["margin"] is not None
    ]
    max_allowed = None
    if capacity_margins:
        max_allowed = max(0.0, min(capacity_margins))

    fail_indices = [i for i, entry in enumerate(evaluation.segment_context) if entry["status"] != "ok"]
    prefix_nodes: Optional[List[int]] = None
    suffix_nodes: Optional[List[int]] = None
    if fail_indices:
        first_fail = fail_indices[0]
        if first_fail > 0:
            prefix_nodes = evaluation.path_nodes[: first_fail + 1]
            if len(prefix_nodes) < 2:
                prefix_nodes = None
        last_fail = fail_indices[-1]
        if last_fail < len(evaluation.segment_context) - 1:
            suffix_nodes = evaluation.path_nodes[last_fail + 1 :]
            if len(suffix_nodes) < 2:
                suffix_nodes = None

    options: Dict[str, str] = {"d": "Drop this service entirely"}
    if max_allowed is not None and max_allowed + 1e-6 < proposal.frequency and max_allowed > 0:
        options["r"] = f"Reduce frequency (maximum {max_allowed:.2f} tph)"
    if prefix_nodes:
        prefix_label = " -> ".join(_format_node(node_id, station_lookup) for node_id in prefix_nodes)
        options["p"] = f"Shorten to {prefix_label}"
    if suffix_nodes:
        suffix_label = " -> ".join(_format_node(node_id, station_lookup) for node_id in suffix_nodes)
        options["s"] = f"Shorten to {suffix_label}"

    if len(options) == 1:
        print("  Only option available: drop the service.")

    while True:
        print("  Available actions:")
        for key, description in options.items():
            print(f"    [{key}] {description}")
        choice = input("  Choose action: ").strip().lower()
        if choice in options:
            break
        print("  Invalid choice. Please try again.")

    if choice == "d":
        del proposals[index]
        print("  Service dropped.")
        return proposals

    if choice == "r":
        if max_allowed is None or max_allowed <= 0:
            print("  Reduction not possible; no capacity margin available. Dropping service.")
            del proposals[index]
            return proposals
        recommended = min(proposal.frequency, max_allowed)
        prompt = input(
            f"  Enter new frequency in tph (<= {max_allowed:.2f}, default {recommended:.2f}): "
        ).strip()
        if prompt:
            try:
                new_frequency = float(prompt)
            except ValueError:
                print("  Invalid input; keeping original frequency.")
                return proposals
        else:
            new_frequency = recommended
        if new_frequency <= 0 or new_frequency - max_allowed > 1e-6:
            print("  Requested frequency exceeds available capacity. No change applied.")
            return proposals
        proposals[index] = ServiceProposal(
            dev_id=proposal.dev_id,
            service=proposal.service,
            frequency=new_frequency,
            from_node=proposal.from_node,
            to_node=proposal.to_node,
            via=proposal.via,
            travel_time_min=proposal.travel_time_min,
            path_length_m=proposal.path_length_m,
        )
        print(f"  Frequency updated to {new_frequency:.2f} tph.")
        return proposals

    if choice in {"p", "s"}:
        nodes = prefix_nodes if choice == "p" else suffix_nodes
        if not nodes:
            print("  Unable to shorten along the chosen direction. No change applied.")
            return proposals
        proposals[index] = _proposal_with_nodes(proposal, nodes)
        print(
            "  Service shortened to: "
            + " -> ".join(_format_node(node_id, station_lookup) for node_id in nodes)
        )
        return proposals

    return proposals


def resolve_capacity_conflicts(plan: InfrastructurePlan, snapshot: CapacitySnapshot) -> InfrastructurePlan:
    if not plan.proposals:
        print("Phase 5: No proposals remaining after benefit screening.")
        return plan

    station_lookup = _build_station_lookup(snapshot.stations)
    pending = list(plan.proposals)

    while pending:
        current_plan = InfrastructurePlan(proposals=pending, source_path=plan.source_path)
        evaluations = evaluate_capacity(current_plan, snapshot)
        problematic = [evaluation for evaluation in evaluations if evaluation.issues]
        if not problematic:
            print("Phase 5: All surviving proposals satisfy capacity limits.")
            return InfrastructurePlan(proposals=pending, source_path=plan.source_path)
        pending = _resolve_single_capacity_issue(problematic[0], pending, station_lookup)

    print("Phase 5: All proposals were removed during capacity resolution.")
    return InfrastructurePlan(proposals=[], source_path=plan.source_path)



def run_capacity_constrained_pipeline() -> None:
    """Entry point for the capacity-aware workflow."""
    config, runtimes = initialize_phase_zero()

    try:
        run_phase_one(config, runtimes)
        run_phase_two(config, runtimes)
        run_phase_three(config, runtimes)
        run_phase_five(config, runtimes)
        run_phase_six(config, runtimes)
        run_phase_seven(config, runtimes)
        run_phase_eight(config, runtimes)
        run_phase_nine(config, runtimes)
        run_phase_ten(config, runtimes)
        print("Phase 10 complete. Pipeline finished.")
        if False:  # Placeholder for upcoming phases once re-enabled.
            snapshot = config.capacity_snapshot or calculate_capacity_snapshot()
            print(
                f"Capacity snapshot stations: {len(snapshot.stations)}, segments: {len(snapshot.segments)}."
            )

            plan = generate_infrastructure_plan()
            print(f"  Stored {len(plan.proposals)} infrastructure proposals for downstream capacity checks.")

            plan, scoring_results = score_developments(plan)
            if scoring_results:
                top_positive = [result for result in scoring_results if result.best_net_benefit > 0]
                print(
                    f"  Benefit screening retained {len(plan.proposals)} proposals "
                    f"({len(top_positive)} developments with positive net benefit)."
                )
                if top_positive:
                    top_positive.sort(key=lambda item: item.best_net_benefit, reverse=True)
                    for entry in top_positive[:5]:
                        print(
                            f"    Development {entry.development_id}: net benefit {entry.best_net_benefit:,.0f} CHF, "
                            f"CBA ratio {entry.best_cba_ratio:.2f}"
                        )
                    if len(top_positive) > 5:
                        print(f"    ... and {len(top_positive) - 5} more with positive benefit.")
            else:
                print("  Benefit screening produced no scoring results.")

            plan = resolve_capacity_conflicts(plan, snapshot)

            final_evaluations = evaluate_capacity(plan, snapshot)
            if final_evaluations:
                exceeded = [
                    evaluation
                    for evaluation in final_evaluations
                    if any(issue.status == "capacity_exceeded" for issue in evaluation.issues)
                ]
                if exceeded:
                    print("  Capacity constraints detected for the following proposals:")
                    for evaluation in exceeded[:5]:
                        detail = ", ".join(
                            f"{issue.start_node}->{issue.end_node} "
                            f"(needs {issue.required_total_tphpd:.1f} vs cap {issue.capacity_tphpd:.1f})"
                            if issue.status == "capacity_exceeded"
                            else f"{issue.start_node}->{issue.end_node} [{issue.status}]"
                            for issue in evaluation.issues
                        )
                        print(
                            f"    Service {evaluation.proposal.service or 'Unnamed'} "
                            f"(dev {evaluation.proposal.dev_id}): {detail}"
                        )
                    if len(exceeded) > 5:
                        print(f"    ... and {len(exceeded) - 5} more.")
                else:
                    print("  No capacity overruns detected for proposed services.")
            else:
                print("  No proposal evaluations were generated.")

    finally:
        runtimes.write(config.runtimes_path)
    # TODO: Phase 5 - continue with evaluation/scenario logic.


if __name__ == "__main__":
    run_capacity_constrained_pipeline()
